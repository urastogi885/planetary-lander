{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "colab.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MoymdnwMng_",
        "colab_type": "code",
        "outputId": "fe82e890-ada5-4981-a035-b2c286740e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcpBTVKRNYgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add necessary headers\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import adam\n",
        "from keras.activations import relu, linear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drUIAuJ4PGtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Render the environment \n",
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlovd0OEOgqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQNetwork:\n",
        "  # A class to implement the deep q-learning algorithm\n",
        "\n",
        "  def __init__(self, state_space, action_space, alpha=0.001, gamma=0.99, epsilon=1.0):\n",
        "    self.state_space = state_space\n",
        "    self.action_space = action_space\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = 0.99\n",
        "    self.epsilon_min = 0.01\n",
        "    self.batch_size = 50\n",
        "    self.memory = deque(maxlen=1000000)\n",
        "    self.model = self.create_model()\n",
        "\n",
        "  def create_model(self):\n",
        "    model = Sequential()\n",
        "    model = Sequential()\n",
        "    model.add(Dense(150, input_dim=self.state_space, activation=relu))\n",
        "    model.add(Dense(120, activation=relu))\n",
        "    model.add(Dense(self.action_space, activation=linear))\n",
        "    model.compile(loss='mse', optimizer=adam(lr=self.alpha))\n",
        "    return model\n",
        "\n",
        "  def get_optimal_action(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return random.randrange(self.action_space)\n",
        "    action_values = self.model.predict(state)\n",
        "    return np.argmax(action_values[0])\n",
        "\n",
        "  def update_memory(self, state, action, reward, done):\n",
        "    self.memory.append((state, action, reward, done))\n",
        "\n",
        "  def replay(self):\n",
        "    if len(self.memory) < self.batch_size:\n",
        "        return\n",
        "\n",
        "    mini_batch = random.sample(self.memory, self.batch_size)\n",
        "    states = np.array([i[0] for i in mini_batch])\n",
        "    actions = np.array([i[1] for i in mini_batch])\n",
        "    rewards = np.array([i[2] for i in mini_batch])\n",
        "    next_states = np.array([i[3] for i in mini_batch])\n",
        "    dones = np.array([i[4] for i in mini_batch])\n",
        "\n",
        "    states = np.squeeze(states)\n",
        "    next_states = np.squeeze(next_states)\n",
        "\n",
        "    targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - dones)\n",
        "    targets_full = self.model.predict_on_batch(states)\n",
        "    ind = np.array([i for i in range(self.batch_size)])\n",
        "    targets_full[[ind], [actions]] = targets\n",
        "\n",
        "    self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}